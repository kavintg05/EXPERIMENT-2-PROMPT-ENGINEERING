# EXP-2-PROMPT-ENGINEERING-

## Aim: 
Comparative Analysis of different types of Prompting patterns and explain with Various Test Scenarios

Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
Analyze the quality, accuracy, and depth of the generated responses.


## Algorithm:
Algorithm: Comparative Analysis of Prompting Patterns

1)Start

2)Define the objective of the experiment: To compare responses generated from unstructured prompts and structured prompts.

3)Select multiple test scenarios such as General Knowledge, Coding, Creative Writing, Academic Explanation, and Summarization.

4)For each scenario, design two types of prompts:
a. Unstructured prompt (broad, vague, open-ended)
b. Structured prompt (clear, specific, refined)

5)Input the unstructured prompt into the AI model and record the response.

6)Input the structured prompt into the AI model and record the response.

7)Evaluate both responses based on accuracy, clarity, depth, and relevance.

8)Tabulate the results for unstructured vs. structured prompts.

9)Analyze the findings and identify which type of prompt produces better responses.

10)Conclude the experiment by summarizing the effectiveness of structured prompts over unstructured prompts.

11)Stop


## Output
https://github.com/kavintg05/EXPERIMENT-2-PROMPT-ENGINEERING/blob/main/prompt-2%20lab.pdf
## Result
The experiment shows that structured prompts consistently produce more accurate, clear, and detailed responses compared to unstructured prompts. Hence, refined prompting patterns are more effective for achieving reliable outputs.
